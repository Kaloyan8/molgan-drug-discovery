{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTobmqu3axab",
        "outputId": "8a858400-5e59-40d3-81d2-06810efa3eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libcairo2-dev is already the newest version (1.16.0-5ubuntu2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y libcairo2-dev\n",
        "!pip -q install rdkit-pypi\n",
        "!pip -q install rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rbt9Lf0Ffj8"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem.Draw import IPythonConsole, MolsToGridImage\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "\n",
        "RDLogger.DisableLog(\"rdApp.*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuLtnekHrHJy"
      },
      "outputs": [],
      "source": [
        "pip -q install Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "d3mfRrs6a0vF",
        "outputId": "202b5032-7b49-43bd-b0d6-cb6740bed11f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SMILES: Oc1cc(F)ccc1-c1nnco1\n",
            "Num heavy atoms: 13\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<rdkit.Chem.rdchem.Mol at 0x7a4f9eb159a0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Code adapated from: https://keras.io/examples/generative/wgan-graphs/\n",
        "##Generate graph from mol\n",
        "\n",
        "csv_path = \"enumerated_smiles.csv\"\n",
        "\n",
        "data = []\n",
        "with open(csv_path, \"r\") as f:\n",
        "    # Skip the header line\n",
        "    next(f)\n",
        "    for line in f:\n",
        "        # Append the entire line (which is the SMILES string) to data\n",
        "        data.append(line.strip())\n",
        "\n",
        "# Let's look at a molecule of the dataset\n",
        "smiles = data[1000]  # Assuming you still want to look at the 1001st molecule\n",
        "print(\"SMILES:\", smiles)\n",
        "molecule = Chem.MolFromSmiles(smiles)\n",
        "print(\"Num heavy atoms:\", molecule.GetNumHeavyAtoms())\n",
        "molecule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "Ix8tkmGibePX",
        "outputId": "e938090e-8988-4ad3-c6c0-0f3677f5a2b0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "image/svg+xml": "<?xml version='1.0' encoding='iso-8859-1'?>\n<svg version='1.1' baseProfile='full'\n              xmlns='http://www.w3.org/2000/svg'\n                      xmlns:rdkit='http://www.rdkit.org/xml'\n                      xmlns:xlink='http://www.w3.org/1999/xlink'\n                  xml:space='preserve'\nwidth='450px' height='150px' viewBox='0 0 450 150'>\n<!-- END OF HEADER -->\n<rect style='opacity:1.0;fill:#FFFFFF;stroke:none' width='450.0' height='150.0' x='0.0' y='0.0'> </rect>\n<path class='bond-0 atom-0 atom-1' d='M 68.9,117.6 L 85.9,122.7' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-0 atom-0 atom-1' d='M 85.9,122.7 L 102.9,127.9' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-1 atom-1 atom-2' d='M 116.8,123.5 L 129.0,112.1' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-1 atom-1 atom-2' d='M 129.0,112.1 L 141.1,100.7' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-2 atom-5 atom-6' d='M 203.7,42.2 L 216.8,29.9' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-2 atom-5 atom-6' d='M 216.8,29.9 L 229.9,17.6' style='fill:none;fill-rule:evenodd;stroke:#33CCCC;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-3 atom-7 atom-8' d='M 213.4,83.9 L 254.4,96.3' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-4 atom-10 atom-11' d='M 323.7,95.0 L 340.4,89.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-4 atom-10 atom-11' d='M 340.4,89.2 L 357.2,83.4' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-5 atom-2 atom-3' d='M 141.1,100.7 L 131.4,59.0' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-5 atom-2 atom-3' d='M 146.6,95.7 L 138.5,61.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-6 atom-2 atom-14' d='M 141.1,100.7 L 182.1,113.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-7 atom-3 atom-4' d='M 131.4,59.0 L 162.7,29.7' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-8 atom-4 atom-5' d='M 162.7,29.7 L 203.7,42.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-8 atom-4 atom-5' d='M 164.4,37.0 L 198.3,47.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-9 atom-5 atom-7' d='M 203.7,42.2 L 213.4,83.9' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-10 atom-7 atom-14' d='M 213.4,83.9 L 182.1,113.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-10 atom-7 atom-14' d='M 206.3,81.7 L 180.4,105.9' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-11 atom-8 atom-9' d='M 254.4,96.3 L 268.5,85.7' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-11 atom-8 atom-9' d='M 268.5,85.7 L 282.5,75.0' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-11 atom-8 atom-9' d='M 262.0,98.6 L 274.2,89.4' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-11 atom-8 atom-9' d='M 274.2,89.4 L 286.4,80.2' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-12 atom-8 atom-13' d='M 254.4,96.3 L 260.1,112.8' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-12 atom-8 atom-13' d='M 260.1,112.8 L 265.8,129.3' style='fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-13 atom-9 atom-10' d='M 294.6,74.7 L 309.1,84.8' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-13 atom-9 atom-10' d='M 309.1,84.8 L 323.7,95.0' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-14 atom-10 atom-12' d='M 323.7,95.0 L 311.3,136.0' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-14 atom-10 atom-12' d='M 316.2,97.6 L 306.5,129.6' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-15 atom-12 atom-13' d='M 311.3,136.0 L 292.5,136.3' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path class='bond-15 atom-12 atom-13' d='M 292.5,136.3 L 273.7,136.7' style='fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n<path d='M 131.9,61.1 L 131.4,59.0 L 133.0,57.5' style='fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n<path d='M 161.1,31.2 L 162.7,29.7 L 164.7,30.3' style='fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n<path d='M 311.9,133.9 L 311.3,136.0 L 310.3,136.0' style='fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n<path d='M 180.1,112.5 L 182.1,113.2 L 183.7,111.7' style='fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n<path class='atom-1' d='M 104.3 130.0\nQ 104.3 127.1, 105.7 125.5\nQ 107.2 123.9, 109.9 123.9\nQ 112.6 123.9, 114.0 125.5\nQ 115.4 127.1, 115.4 130.0\nQ 115.4 133.0, 114.0 134.7\nQ 112.5 136.3, 109.9 136.3\nQ 107.2 136.3, 105.7 134.7\nQ 104.3 133.0, 104.3 130.0\nM 109.9 135.0\nQ 111.7 135.0, 112.7 133.7\nQ 113.7 132.5, 113.7 130.0\nQ 113.7 127.7, 112.7 126.5\nQ 111.7 125.2, 109.9 125.2\nQ 108.0 125.2, 107.0 126.4\nQ 106.0 127.6, 106.0 130.0\nQ 106.0 132.5, 107.0 133.7\nQ 108.0 135.0, 109.9 135.0\n' fill='#FF0000'/>\n<path class='atom-6' d='M 231.3 6.8\nL 238.6 6.8\nL 238.6 8.2\nL 233.0 8.2\nL 233.0 11.9\nL 237.9 11.9\nL 237.9 13.3\nL 233.0 13.3\nL 233.0 19.0\nL 231.3 19.0\nL 231.3 6.8\n' fill='#33CCCC'/>\n<path class='atom-9' d='M 285.9 64.4\nL 289.9 70.8\nQ 290.2 71.5, 290.9 72.6\nQ 291.5 73.8, 291.5 73.8\nL 291.5 64.4\nL 293.2 64.4\nL 293.2 76.5\nL 291.5 76.5\nL 287.2 69.5\nQ 286.7 68.7, 286.2 67.8\nQ 285.7 66.8, 285.5 66.5\nL 285.5 76.5\nL 284.0 76.5\nL 284.0 64.4\nL 285.9 64.4\n' fill='#0000FF'/>\n<path class='atom-11' d='M 358.6 81.0\nQ 358.6 78.1, 360.0 76.4\nQ 361.5 74.8, 364.2 74.8\nQ 366.9 74.8, 368.3 76.4\nQ 369.7 78.1, 369.7 81.0\nQ 369.7 83.9, 368.3 85.6\nQ 366.8 87.3, 364.2 87.3\nQ 361.5 87.3, 360.0 85.6\nQ 358.6 83.9, 358.6 81.0\nM 364.2 85.9\nQ 366.0 85.9, 367.0 84.7\nQ 368.0 83.4, 368.0 81.0\nQ 368.0 78.6, 367.0 77.4\nQ 366.0 76.2, 364.2 76.2\nQ 362.3 76.2, 361.3 77.4\nQ 360.3 78.6, 360.3 81.0\nQ 360.3 83.4, 361.3 84.7\nQ 362.3 85.9, 364.2 85.9\n' fill='#FF0000'/>\n<path class='atom-11' d='M 371.6 75.0\nL 373.3 75.0\nL 373.3 80.1\nL 379.5 80.1\nL 379.5 75.0\nL 381.1 75.0\nL 381.1 87.1\nL 379.5 87.1\nL 379.5 81.5\nL 373.3 81.5\nL 373.3 87.1\nL 371.6 87.1\nL 371.6 75.0\n' fill='#FF0000'/>\n<path class='atom-13' d='M 265.0 141.0\nQ 265.1 141.0, 265.7 141.3\nQ 266.3 141.5, 266.9 141.7\nQ 267.5 141.8, 268.1 141.8\nQ 269.3 141.8, 270.0 141.2\nQ 270.6 140.7, 270.6 139.7\nQ 270.6 139.0, 270.3 138.6\nQ 270.0 138.2, 269.4 138.0\nQ 268.9 137.8, 268.1 137.5\nQ 267.0 137.2, 266.3 136.9\nQ 265.7 136.6, 265.2 135.9\nQ 264.8 135.3, 264.8 134.2\nQ 264.8 132.6, 265.8 131.7\nQ 266.9 130.8, 268.9 130.8\nQ 270.3 130.8, 271.9 131.4\nL 271.5 132.7\nQ 270.1 132.1, 269.0 132.1\nQ 267.8 132.1, 267.1 132.6\nQ 266.5 133.1, 266.5 134.0\nQ 266.5 134.6, 266.8 135.0\nQ 267.2 135.4, 267.7 135.6\nQ 268.2 135.8, 269.0 136.1\nQ 270.1 136.4, 270.7 136.8\nQ 271.4 137.1, 271.8 137.8\nQ 272.3 138.5, 272.3 139.7\nQ 272.3 141.4, 271.2 142.3\nQ 270.1 143.2, 268.2 143.2\nQ 267.1 143.2, 266.3 142.9\nQ 265.5 142.7, 264.5 142.3\nL 265.0 141.0\n' fill='#CCCC00'/>\n</svg>\n",
            "text/html": [
              "<?xml version='1.0' encoding='iso-8859-1'?>\n",
              "<svg version='1.1' baseProfile='full'\n",
              "              xmlns='http://www.w3.org/2000/svg'\n",
              "                      xmlns:rdkit='http://www.rdkit.org/xml'\n",
              "                      xmlns:xlink='http://www.w3.org/1999/xlink'\n",
              "                  xml:space='preserve'\n",
              "width='450px' height='150px' viewBox='0 0 450 150'>\n",
              "<!-- END OF HEADER -->\n",
              "<rect style='opacity:1.0;fill:#FFFFFF;stroke:none' width='450.0' height='150.0' x='0.0' y='0.0'> </rect>\n",
              "<path class='bond-0 atom-0 atom-1' d='M 68.9,117.6 L 85.9,122.7' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-0 atom-0 atom-1' d='M 85.9,122.7 L 102.9,127.9' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-1 atom-1 atom-2' d='M 116.8,123.5 L 129.0,112.1' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-1 atom-1 atom-2' d='M 129.0,112.1 L 141.1,100.7' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-2 atom-5 atom-6' d='M 203.7,42.2 L 216.8,29.9' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-2 atom-5 atom-6' d='M 216.8,29.9 L 229.9,17.6' style='fill:none;fill-rule:evenodd;stroke:#33CCCC;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-3 atom-7 atom-8' d='M 213.4,83.9 L 254.4,96.3' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-4 atom-10 atom-11' d='M 323.7,95.0 L 340.4,89.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-4 atom-10 atom-11' d='M 340.4,89.2 L 357.2,83.4' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-5 atom-2 atom-3' d='M 141.1,100.7 L 131.4,59.0' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-5 atom-2 atom-3' d='M 146.6,95.7 L 138.5,61.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-6 atom-2 atom-14' d='M 141.1,100.7 L 182.1,113.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-7 atom-3 atom-4' d='M 131.4,59.0 L 162.7,29.7' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-8 atom-4 atom-5' d='M 162.7,29.7 L 203.7,42.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-8 atom-4 atom-5' d='M 164.4,37.0 L 198.3,47.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-9 atom-5 atom-7' d='M 203.7,42.2 L 213.4,83.9' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-10 atom-7 atom-14' d='M 213.4,83.9 L 182.1,113.2' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-10 atom-7 atom-14' d='M 206.3,81.7 L 180.4,105.9' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-11 atom-8 atom-9' d='M 254.4,96.3 L 268.5,85.7' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-11 atom-8 atom-9' d='M 268.5,85.7 L 282.5,75.0' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-11 atom-8 atom-9' d='M 262.0,98.6 L 274.2,89.4' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-11 atom-8 atom-9' d='M 274.2,89.4 L 286.4,80.2' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-12 atom-8 atom-13' d='M 254.4,96.3 L 260.1,112.8' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-12 atom-8 atom-13' d='M 260.1,112.8 L 265.8,129.3' style='fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-13 atom-9 atom-10' d='M 294.6,74.7 L 309.1,84.8' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-13 atom-9 atom-10' d='M 309.1,84.8 L 323.7,95.0' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-14 atom-10 atom-12' d='M 323.7,95.0 L 311.3,136.0' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-14 atom-10 atom-12' d='M 316.2,97.6 L 306.5,129.6' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-15 atom-12 atom-13' d='M 311.3,136.0 L 292.5,136.3' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path class='bond-15 atom-12 atom-13' d='M 292.5,136.3 L 273.7,136.7' style='fill:none;fill-rule:evenodd;stroke:#CCCC00;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
              "<path d='M 131.9,61.1 L 131.4,59.0 L 133.0,57.5' style='fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n",
              "<path d='M 161.1,31.2 L 162.7,29.7 L 164.7,30.3' style='fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n",
              "<path d='M 311.9,133.9 L 311.3,136.0 L 310.3,136.0' style='fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n",
              "<path d='M 180.1,112.5 L 182.1,113.2 L 183.7,111.7' style='fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;' />\n",
              "<path class='atom-1' d='M 104.3 130.0\n",
              "Q 104.3 127.1, 105.7 125.5\n",
              "Q 107.2 123.9, 109.9 123.9\n",
              "Q 112.6 123.9, 114.0 125.5\n",
              "Q 115.4 127.1, 115.4 130.0\n",
              "Q 115.4 133.0, 114.0 134.7\n",
              "Q 112.5 136.3, 109.9 136.3\n",
              "Q 107.2 136.3, 105.7 134.7\n",
              "Q 104.3 133.0, 104.3 130.0\n",
              "M 109.9 135.0\n",
              "Q 111.7 135.0, 112.7 133.7\n",
              "Q 113.7 132.5, 113.7 130.0\n",
              "Q 113.7 127.7, 112.7 126.5\n",
              "Q 111.7 125.2, 109.9 125.2\n",
              "Q 108.0 125.2, 107.0 126.4\n",
              "Q 106.0 127.6, 106.0 130.0\n",
              "Q 106.0 132.5, 107.0 133.7\n",
              "Q 108.0 135.0, 109.9 135.0\n",
              "' fill='#FF0000'/>\n",
              "<path class='atom-6' d='M 231.3 6.8\n",
              "L 238.6 6.8\n",
              "L 238.6 8.2\n",
              "L 233.0 8.2\n",
              "L 233.0 11.9\n",
              "L 237.9 11.9\n",
              "L 237.9 13.3\n",
              "L 233.0 13.3\n",
              "L 233.0 19.0\n",
              "L 231.3 19.0\n",
              "L 231.3 6.8\n",
              "' fill='#33CCCC'/>\n",
              "<path class='atom-9' d='M 285.9 64.4\n",
              "L 289.9 70.8\n",
              "Q 290.2 71.5, 290.9 72.6\n",
              "Q 291.5 73.8, 291.5 73.8\n",
              "L 291.5 64.4\n",
              "L 293.2 64.4\n",
              "L 293.2 76.5\n",
              "L 291.5 76.5\n",
              "L 287.2 69.5\n",
              "Q 286.7 68.7, 286.2 67.8\n",
              "Q 285.7 66.8, 285.5 66.5\n",
              "L 285.5 76.5\n",
              "L 284.0 76.5\n",
              "L 284.0 64.4\n",
              "L 285.9 64.4\n",
              "' fill='#0000FF'/>\n",
              "<path class='atom-11' d='M 358.6 81.0\n",
              "Q 358.6 78.1, 360.0 76.4\n",
              "Q 361.5 74.8, 364.2 74.8\n",
              "Q 366.9 74.8, 368.3 76.4\n",
              "Q 369.7 78.1, 369.7 81.0\n",
              "Q 369.7 83.9, 368.3 85.6\n",
              "Q 366.8 87.3, 364.2 87.3\n",
              "Q 361.5 87.3, 360.0 85.6\n",
              "Q 358.6 83.9, 358.6 81.0\n",
              "M 364.2 85.9\n",
              "Q 366.0 85.9, 367.0 84.7\n",
              "Q 368.0 83.4, 368.0 81.0\n",
              "Q 368.0 78.6, 367.0 77.4\n",
              "Q 366.0 76.2, 364.2 76.2\n",
              "Q 362.3 76.2, 361.3 77.4\n",
              "Q 360.3 78.6, 360.3 81.0\n",
              "Q 360.3 83.4, 361.3 84.7\n",
              "Q 362.3 85.9, 364.2 85.9\n",
              "' fill='#FF0000'/>\n",
              "<path class='atom-11' d='M 371.6 75.0\n",
              "L 373.3 75.0\n",
              "L 373.3 80.1\n",
              "L 379.5 80.1\n",
              "L 379.5 75.0\n",
              "L 381.1 75.0\n",
              "L 381.1 87.1\n",
              "L 379.5 87.1\n",
              "L 379.5 81.5\n",
              "L 373.3 81.5\n",
              "L 373.3 87.1\n",
              "L 371.6 87.1\n",
              "L 371.6 75.0\n",
              "' fill='#FF0000'/>\n",
              "<path class='atom-13' d='M 265.0 141.0\n",
              "Q 265.1 141.0, 265.7 141.3\n",
              "Q 266.3 141.5, 266.9 141.7\n",
              "Q 267.5 141.8, 268.1 141.8\n",
              "Q 269.3 141.8, 270.0 141.2\n",
              "Q 270.6 140.7, 270.6 139.7\n",
              "Q 270.6 139.0, 270.3 138.6\n",
              "Q 270.0 138.2, 269.4 138.0\n",
              "Q 268.9 137.8, 268.1 137.5\n",
              "Q 267.0 137.2, 266.3 136.9\n",
              "Q 265.7 136.6, 265.2 135.9\n",
              "Q 264.8 135.3, 264.8 134.2\n",
              "Q 264.8 132.6, 265.8 131.7\n",
              "Q 266.9 130.8, 268.9 130.8\n",
              "Q 270.3 130.8, 271.9 131.4\n",
              "L 271.5 132.7\n",
              "Q 270.1 132.1, 269.0 132.1\n",
              "Q 267.8 132.1, 267.1 132.6\n",
              "Q 266.5 133.1, 266.5 134.0\n",
              "Q 266.5 134.6, 266.8 135.0\n",
              "Q 267.2 135.4, 267.7 135.6\n",
              "Q 268.2 135.8, 269.0 136.1\n",
              "Q 270.1 136.4, 270.7 136.8\n",
              "Q 271.4 137.1, 271.8 137.8\n",
              "Q 272.3 138.5, 272.3 139.7\n",
              "Q 272.3 141.4, 271.2 142.3\n",
              "Q 270.1 143.2, 268.2 143.2\n",
              "Q 267.1 143.2, 266.3 142.9\n",
              "Q 265.5 142.7, 264.5 142.3\n",
              "L 265.0 141.0\n",
              "' fill='#CCCC00'/>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<rdkit.Chem.rdchem.RWMol at 0x7a4f06a701d0>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "atom_mapping = {\n",
        "    \"C\": 0,   0: \"C\",\n",
        "    \"N\": 1,   1: \"N\",\n",
        "    \"O\": 2,   2: \"O\",\n",
        "    \"F\": 3,   3: \"F\",\n",
        "    \"Cl\": 4,  4: \"Cl\",\n",
        "    \"Br\": 5,  5: \"Br\",\n",
        "    \"I\": 6,   6: \"I\",\n",
        "    \"H\": 7,   7: \"H\",\n",
        "    \"P\": 8,   8: \"P\",\n",
        "    \"S\": 9,   9: \"S\"\n",
        "}\n",
        "\n",
        "bond_mapping = {\n",
        "    \"SINGLE\": 0,   0: Chem.BondType.SINGLE,\n",
        "    \"DOUBLE\": 1,   1: Chem.BondType.DOUBLE,\n",
        "    \"TRIPLE\": 2,   2: Chem.BondType.TRIPLE,\n",
        "    \"AROMATIC\": 3, 3: Chem.BondType.AROMATIC,\n",
        "}\n",
        "\n",
        "NUM_ATOMS = 30 # Maximum number of atoms (can be adjusted)\n",
        "ATOM_DIM = 10 + 1  # Updated to 10 atom types + 1 for \"no atom\"\n",
        "BOND_DIM = 4 + 1  # Number of bond types remains the same\n",
        "LATENT_DIM = 256  # Latent space size remains the same\n",
        "\n",
        "\n",
        "\n",
        "def smiles_to_graph(smiles):\n",
        "    # Converts SMILES to molecule object\n",
        "    molecule = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Initialize adjacency and feature tensor\n",
        "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
        "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
        "\n",
        "    # loop over each atom in molecule\n",
        "    for atom in molecule.GetAtoms():\n",
        "        i = atom.GetIdx()\n",
        "        atom_type = atom_mapping[atom.GetSymbol()]\n",
        "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
        "        # loop over one-hop neighbors\n",
        "        for neighbor in atom.GetNeighbors():\n",
        "            j = neighbor.GetIdx()\n",
        "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
        "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
        "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
        "\n",
        "    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n",
        "    # Notice: channels-first\n",
        "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
        "\n",
        "    # Where no atom, add 1 to last column (indicating \"non-atom\")\n",
        "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
        "\n",
        "    return adjacency, features\n",
        "\n",
        "\n",
        "def graph_to_molecule(graph):\n",
        "    # Unpack graph\n",
        "    adjacency, features = graph\n",
        "\n",
        "    # RWMol is a molecule object intended to be edited\n",
        "    molecule = Chem.RWMol()\n",
        "\n",
        "    # Remove \"no atoms\" & atoms with no bonds\n",
        "    keep_idx = np.where(\n",
        "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
        "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
        "    )[0]\n",
        "    features = features[keep_idx]\n",
        "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
        "\n",
        "    # Add atoms to molecule\n",
        "    for atom_type_idx in np.argmax(features, axis=1):\n",
        "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
        "        _ = molecule.AddAtom(atom)\n",
        "\n",
        "    # Add bonds between atoms in molecule; based on the upper triangles\n",
        "    # of the [symmetric] adjacency tensor\n",
        "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
        "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
        "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
        "            continue\n",
        "        bond_type = bond_mapping[bond_ij]\n",
        "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
        "\n",
        "    # Sanitize the molecule; for more information on sanitization, see\n",
        "    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
        "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
        "    # Let's be strict. If sanitization fails, return None\n",
        "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
        "        return None\n",
        "\n",
        "    return molecule\n",
        "\n",
        "\n",
        "# Test helper functions\n",
        "graph_to_molecule(smiles_to_graph(smiles))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srhLGJM4roXF",
        "outputId": "b73c9e21-87e3-4dbe-9cd8-3c36bc01d2ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "adjacency_tensor.shape = (15696, 5, 30, 30)\n",
            "feature_tensor.shape = (15696, 30, 11)\n"
          ]
        }
      ],
      "source": [
        "adjacency_tensor, feature_tensor = [], []\n",
        "for smiles in data:\n",
        "    adjacency, features = smiles_to_graph(smiles)\n",
        "    adjacency_tensor.append(adjacency)\n",
        "    feature_tensor.append(features)\n",
        "\n",
        "adjacency_tensor = np.array(adjacency_tensor)\n",
        "feature_tensor = np.array(feature_tensor)\n",
        "\n",
        "print(\"adjacency_tensor.shape =\", adjacency_tensor.shape)\n",
        "print(\"feature_tensor.shape =\", feature_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR2Oi2nZbo01"
      },
      "outputs": [],
      "source": [
        "def GraphGenerator(\n",
        "    dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape,\n",
        "):\n",
        "    z = keras.layers.Input(shape=(LATENT_DIM,))\n",
        "    # Propagate through one or more densely connected layers\n",
        "    x = z\n",
        "    for units in dense_units:\n",
        "        x = keras.layers.Dense(units, activation=\"tanh\")(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n",
        "    # Calculate the total number of elements in adjacency_shape\n",
        "    units_adjacency = np.prod(adjacency_shape) # Changed this line\n",
        "    x_adjacency = keras.layers.Dense(units_adjacency)(x) # Changed this line\n",
        "    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)\n",
        "    # Symmetrify tensors in the last two dimensions\n",
        "    # Wrap tf.transpose within a Lambda layer\n",
        "    x_adjacency = keras.layers.Lambda(lambda x: (x + tf.transpose(x, (0, 1, 3, 2))) / 2)(x_adjacency)\n",
        "    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)\n",
        "\n",
        "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
        "    # Calculate the total number of elements in feature_shape\n",
        "    units_feature = np.prod(feature_shape) # Changed this line\n",
        "    x_features = keras.layers.Dense(units_feature)(x) # Changed this line\n",
        "    x_features = keras.layers.Reshape(feature_shape)(x_features)\n",
        "    x_features = keras.layers.Softmax(axis=2)(x_features)\n",
        "\n",
        "    return keras.Model(inputs=z, outputs=[x_adjacency, x_features], name=\"Generator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "8aR2dUiPbsv3",
        "outputId": "70abbd72-74bb-4b61-9d10-cbb8df29a88f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_9             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_10            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relational_graph_conv_la… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">7,040</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RelationalGraphConvLaye…</span> │                        │                │ input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relational_graph_conv_la… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RelationalGraphConvLaye…</span> │                        │                │ relational_graph_conv… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relational_graph_conv_la… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RelationalGraphConvLaye…</span> │                        │                │ relational_graph_conv… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relational_graph_conv_la… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920</span> │ input_layer_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RelationalGraphConvLaye…</span> │                        │                │ relational_graph_conv… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ relational_graph_conv… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_9             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_10            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m11\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relational_graph_conv_la… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │          \u001b[38;5;34m7,040\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mRelationalGraphConvLaye…\u001b[0m │                        │                │ input_layer_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relational_graph_conv_la… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m81,920\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mRelationalGraphConvLaye…\u001b[0m │                        │                │ relational_graph_conv… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relational_graph_conv_la… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m81,920\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mRelationalGraphConvLaye…\u001b[0m │                        │                │ relational_graph_conv… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ relational_graph_conv_la… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │         \u001b[38;5;34m81,920\u001b[0m │ input_layer_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mRelationalGraphConvLaye…\u001b[0m │                        │                │ relational_graph_conv… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ relational_graph_conv… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m66,048\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m262,656\u001b[0m │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m513\u001b[0m │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">582,017</span> (2.22 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m582,017\u001b[0m (2.22 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">582,017</span> (2.22 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m582,017\u001b[0m (2.22 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "class RelationalGraphConvLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        units=128,\n",
        "        activation=\"relu\",\n",
        "        use_bias=False,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        bond_dim = input_shape[0][1]\n",
        "        atom_dim = input_shape[1][2]\n",
        "\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(bond_dim, atom_dim, self.units),\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            trainable=True,\n",
        "            name=\"W\",\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(bond_dim, 1, self.units),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                trainable=True,\n",
        "                name=\"b\",\n",
        "                dtype=tf.float32,\n",
        "            )\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        adjacency, features = inputs\n",
        "        # Aggregate information from neighbors\n",
        "        x = tf.matmul(adjacency, features[:, None, :, :])\n",
        "        # Apply linear transformation\n",
        "        x = tf.matmul(x, self.kernel)\n",
        "        if self.use_bias:\n",
        "            x += self.bias\n",
        "        # Reduce bond types dim\n",
        "        x_reduced = tf.reduce_sum(x, axis=1)\n",
        "        # Apply non-linear transformation\n",
        "        return self.activation(x_reduced)\n",
        "\n",
        "\n",
        "def GraphDiscriminator(\n",
        "    gconv_units, dense_units, dropout_rate, adjacency_shape, feature_shape\n",
        "):\n",
        "\n",
        "    adjacency = keras.layers.Input(shape=adjacency_shape)\n",
        "    features = keras.layers.Input(shape=feature_shape)\n",
        "\n",
        "    # Propagate through one or more graph convolutional layers\n",
        "    features_transformed = features\n",
        "    for units in gconv_units:\n",
        "        features_transformed = RelationalGraphConvLayer(units)(\n",
        "            [adjacency, features_transformed]\n",
        "        )\n",
        "\n",
        "    # Reduce 2-D representation of molecule to 1-D\n",
        "    x = keras.layers.GlobalAveragePooling1D()(features_transformed)\n",
        "\n",
        "    # Propagate through one or more densely connected layers\n",
        "    for units in dense_units:\n",
        "        x = keras.layers.Dense(units, activation=\"relu\")(x)\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # For each molecule, output a single scalar value expressing the\n",
        "    # \"realness\" of the inputted molecule\n",
        "    x_out = keras.layers.Dense(1, dtype=\"float32\")(x)\n",
        "\n",
        "    return keras.Model(inputs=[adjacency, features], outputs=x_out)\n",
        "\n",
        "\n",
        "discriminator = GraphDiscriminator(\n",
        "    gconv_units=[128, 128, 128, 128],\n",
        "    dense_units=[512, 512],\n",
        "    dropout_rate=0.2,\n",
        "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
        "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
        ")\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTduHFKNbzkZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphWGAN(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator,\n",
        "        discriminator,\n",
        "        discriminator_steps=1,\n",
        "        generator_steps=1,\n",
        "        gp_weight=10,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.discriminator_steps = discriminator_steps\n",
        "        self.generator_steps = generator_steps\n",
        "        self.gp_weight = gp_weight\n",
        "        self.latent_dim = self.generator.input_shape[-1]\n",
        "\n",
        "    def compile(self, optimizer_generator, optimizer_discriminator, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.optimizer_generator = optimizer_generator\n",
        "        self.optimizer_discriminator = optimizer_discriminator\n",
        "        self.metric_generator = keras.metrics.Mean(name=\"loss_gen\")\n",
        "        self.metric_discriminator = keras.metrics.Mean(name=\"loss_dis\")\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "\n",
        "        if isinstance(inputs[0], tuple):\n",
        "            inputs = inputs[0]\n",
        "\n",
        "        graph_real = inputs\n",
        "\n",
        "        self.batch_size = tf.shape(inputs[0])[0]\n",
        "\n",
        "        # Train the discriminator for one or more steps\n",
        "        for _ in range(self.discriminator_steps):\n",
        "            z = tf.random.normal((self.batch_size, self.latent_dim))\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                graph_generated = self.generator(z, training=True)\n",
        "                loss = self._loss_discriminator(graph_real, graph_generated)\n",
        "\n",
        "            grads = tape.gradient(loss, self.discriminator.trainable_weights)\n",
        "            self.optimizer_discriminator.apply_gradients(\n",
        "                zip(grads, self.discriminator.trainable_weights)\n",
        "            )\n",
        "            self.metric_discriminator.update_state(loss)\n",
        "\n",
        "        # Train the generator for one or more steps\n",
        "        for _ in range(self.generator_steps):\n",
        "            z = tf.random.normal((self.batch_size, self.latent_dim))\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                graph_generated = self.generator(z, training=True)\n",
        "                loss = self._loss_generator(graph_generated)\n",
        "\n",
        "                grads = tape.gradient(loss, self.generator.trainable_weights)\n",
        "                self.optimizer_generator.apply_gradients(\n",
        "                    zip(grads, self.generator.trainable_weights)\n",
        "                )\n",
        "                self.metric_generator.update_state(loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def _loss_discriminator(self, graph_real, graph_generated):\n",
        "        logits_real = self.discriminator(graph_real, training=True)\n",
        "        logits_generated = self.discriminator(graph_generated, training=True)\n",
        "        loss = tf.reduce_mean(logits_generated) - tf.reduce_mean(logits_real)\n",
        "        loss_gp = self._gradient_penalty(graph_real, graph_generated)\n",
        "        return loss + loss_gp * self.gp_weight\n",
        "\n",
        "    def _loss_generator(self, graph_generated):\n",
        "        logits_generated = self.discriminator(graph_generated, training=True)\n",
        "        return -tf.reduce_mean(logits_generated)\n",
        "\n",
        "    def _gradient_penalty(self, graph_real, graph_generated):\n",
        "        # Unpack graphs\n",
        "        adjacency_real, features_real = graph_real\n",
        "        adjacency_generated, features_generated = graph_generated\n",
        "\n",
        "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
        "        alpha = tf.random.uniform([self.batch_size])\n",
        "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
        "        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n",
        "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))\n",
        "        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n",
        "\n",
        "        # Compute the logits of interpolated graphs\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(adjacency_interp)\n",
        "            tape.watch(features_interp)\n",
        "            logits = self.discriminator(\n",
        "                [adjacency_interp, features_interp], training=True\n",
        "            )\n",
        "\n",
        "        # Compute the gradients with respect to the interpolated graphs\n",
        "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
        "        # Compute the gradient penalty\n",
        "        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2\n",
        "        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2\n",
        "        return tf.reduce_mean(\n",
        "            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))\n",
        "            + tf.reduce_mean(grads_features_penalty, axis=(-1))\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "50d14SHvb31s",
        "outputId": "5341158c-5beb-4f59-9a59-58e17bb58f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 319.8887 - loss_gen: 47.7486\n",
            "Epoch 2/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 23.7723 - loss_gen: -1.2935\n",
            "Epoch 3/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.0000e+00 - loss_dis: -0.7986 - loss_gen: -0.9556\n",
            "Epoch 4/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 1.1705 - loss_gen: -2.0032\n",
            "Epoch 5/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 0.6501 - loss_gen: -1.7427\n",
            "Epoch 6/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.3135 - loss_gen: -2.7986\n",
            "Epoch 7/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.9003 - loss_gen: -5.3842\n",
            "Epoch 8/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.5505 - loss_gen: -4.6231\n",
            "Epoch 9/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.5349 - loss_gen: -3.8178\n",
            "Epoch 10/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.5249 - loss_gen: -4.4156\n",
            "Epoch 11/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.4377 - loss_gen: -3.0091\n",
            "Epoch 12/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.2990 - loss_gen: -3.3443\n",
            "Epoch 13/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.8263 - loss_gen: -4.1543\n",
            "Epoch 14/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -0.8608 - loss_gen: -3.8533\n",
            "Epoch 15/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -1.3618 - loss_gen: -4.5670\n",
            "Epoch 16/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -1.3395 - loss_gen: -5.3826\n",
            "Epoch 17/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -2.2918 - loss_gen: -6.2659\n",
            "Epoch 18/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -2.6210 - loss_gen: -5.2035\n",
            "Epoch 19/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -2.1164 - loss_gen: -6.5438\n",
            "Epoch 20/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -1.7728 - loss_gen: -6.6868\n",
            "Epoch 21/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -2.1453 - loss_gen: -6.5389\n",
            "Epoch 22/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -1.3926 - loss_gen: -7.7276\n",
            "Epoch 23/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -2.9928 - loss_gen: -8.9602\n",
            "Epoch 24/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -2.9765 - loss_gen: -8.9043\n",
            "Epoch 25/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -3.2185 - loss_gen: -7.9151\n",
            "Epoch 26/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -3.2317 - loss_gen: -7.2967\n",
            "Epoch 27/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -3.5568 - loss_gen: -7.0442\n",
            "Epoch 28/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 2.4018 - loss_gen: -8.6620\n",
            "Epoch 29/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 4.7235 - loss_gen: -7.2038\n",
            "Epoch 30/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -4.1845 - loss_gen: -6.5571\n",
            "Epoch 31/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -4.5396 - loss_gen: -7.3152\n",
            "Epoch 32/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -4.6281 - loss_gen: -7.6705\n",
            "Epoch 33/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -5.3544 - loss_gen: -7.7584\n",
            "Epoch 34/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -5.4619 - loss_gen: -7.0166\n",
            "Epoch 35/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -5.3697 - loss_gen: -7.8451\n",
            "Epoch 36/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -5.6309 - loss_gen: -7.4835\n",
            "Epoch 37/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -6.5199 - loss_gen: -7.2962\n",
            "Epoch 38/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -6.1069 - loss_gen: -8.6987\n",
            "Epoch 39/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -6.4882 - loss_gen: -6.3702\n",
            "Epoch 40/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -6.9305 - loss_gen: -5.3906\n",
            "Epoch 41/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.0696 - loss_gen: -4.8920\n",
            "Epoch 42/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.1716 - loss_gen: -5.1134\n",
            "Epoch 43/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.9376 - loss_gen: -4.4800\n",
            "Epoch 44/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.6090 - loss_gen: -4.7725\n",
            "Epoch 45/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.0277 - loss_gen: -4.8799\n",
            "Epoch 46/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.2218 - loss_gen: -6.0170\n",
            "Epoch 47/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.5364 - loss_gen: -4.3888\n",
            "Epoch 48/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.9502 - loss_gen: -4.0243\n",
            "Epoch 49/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -9.4920 - loss_gen: -3.4131\n",
            "Epoch 50/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -10.1683 - loss_gen: -3.2521\n",
            "Epoch 51/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -10.2959 - loss_gen: -4.3758\n",
            "Epoch 52/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -9.9078 - loss_gen: -4.6294\n",
            "Epoch 53/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -4.1000 - loss_gen: -6.2164\n",
            "Epoch 54/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.8711 - loss_gen: -7.1043\n",
            "Epoch 55/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -9.0143 - loss_gen: -5.1404\n",
            "Epoch 56/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.7599 - loss_gen: -3.4367\n",
            "Epoch 57/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -6.1611 - loss_gen: -5.4613\n",
            "Epoch 58/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.4093 - loss_gen: -3.0516\n",
            "Epoch 59/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.1593 - loss_gen: -5.7647\n",
            "Epoch 60/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -9.4337 - loss_gen: -7.9057\n",
            "Epoch 61/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.0633 - loss_gen: -6.2185\n",
            "Epoch 62/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 1325.9961 - loss_gen: -7.3645\n",
            "Epoch 63/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.4958 - loss_gen: -6.3749\n",
            "Epoch 64/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.0087 - loss_gen: -6.8365\n",
            "Epoch 65/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.7164 - loss_gen: -7.3756\n",
            "Epoch 66/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.9065 - loss_gen: -8.1604\n",
            "Epoch 67/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.5417 - loss_gen: -8.3644\n",
            "Epoch 68/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.0763 - loss_gen: -8.1245\n",
            "Epoch 69/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.1265 - loss_gen: -7.5390\n",
            "Epoch 70/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -6.0087 - loss_gen: -7.2974\n",
            "Epoch 71/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.1818 - loss_gen: -5.8683\n",
            "Epoch 72/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.1616 - loss_gen: -5.7442\n",
            "Epoch 73/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -7.8692 - loss_gen: -6.8524\n",
            "Epoch 74/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.3177 - loss_gen: -7.0135\n",
            "Epoch 75/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.5562 - loss_gen: -6.3087\n",
            "Epoch 76/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.3786 - loss_gen: -8.0112\n",
            "Epoch 77/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -8.6315 - loss_gen: -7.5750\n",
            "Epoch 78/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -9.2323 - loss_gen: -7.4621\n",
            "Epoch 79/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -9.6022 - loss_gen: -7.3437\n",
            "Epoch 80/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -10.1953 - loss_gen: -7.3242\n",
            "Epoch 81/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -10.0157 - loss_gen: -7.4895\n",
            "Epoch 82/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -10.2931 - loss_gen: -7.4658\n",
            "Epoch 83/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -10.4863 - loss_gen: -6.4701\n",
            "Epoch 84/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.1938 - loss_gen: -6.8031\n",
            "Epoch 85/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.4058 - loss_gen: -6.4184\n",
            "Epoch 86/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.7249 - loss_gen: -6.3198\n",
            "Epoch 87/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.3163 - loss_gen: -6.6503\n",
            "Epoch 88/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.7353 - loss_gen: -7.7537\n",
            "Epoch 89/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.8273 - loss_gen: -7.8972\n",
            "Epoch 90/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.8480 - loss_gen: -7.5811\n",
            "Epoch 91/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.9097 - loss_gen: -7.6667\n",
            "Epoch 92/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.9664 - loss_gen: 11.4587\n",
            "Epoch 93/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.7325 - loss_gen: -6.0554\n",
            "Epoch 94/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.2940 - loss_gen: -6.0922\n",
            "Epoch 95/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.1420 - loss_gen: -5.8890\n",
            "Epoch 96/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.5125 - loss_gen: -7.3343\n",
            "Epoch 97/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.4098 - loss_gen: -2.6587\n",
            "Epoch 98/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.3492 - loss_gen: -6.1256\n",
            "Epoch 99/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 6395.6826 - loss_gen: -10.4069\n",
            "Epoch 100/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.0928 - loss_gen: -8.4693\n",
            "Epoch 101/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.1206 - loss_gen: -8.3485\n",
            "Epoch 102/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.3099 - loss_gen: -9.1639\n",
            "Epoch 103/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.5729 - loss_gen: -7.9988\n",
            "Epoch 104/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.2181 - loss_gen: -8.1612\n",
            "Epoch 105/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 220.0459 - loss_gen: -8.3203\n",
            "Epoch 106/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.5094 - loss_gen: -9.7347\n",
            "Epoch 107/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.8447 - loss_gen: -8.5553\n",
            "Epoch 108/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.9552 - loss_gen: -8.0564\n",
            "Epoch 109/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.0542 - loss_gen: -8.5818\n",
            "Epoch 110/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.0612 - loss_gen: -8.5437\n",
            "Epoch 111/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.8540 - loss_gen: -8.5042\n",
            "Epoch 112/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.8866 - loss_gen: -9.6048\n",
            "Epoch 113/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.7348 - loss_gen: -9.1511\n",
            "Epoch 114/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 0.0000e+00 - loss_dis: -12.0750 - loss_gen: -8.2821\n",
            "Epoch 115/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.8228 - loss_gen: -5.6732\n",
            "Epoch 116/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.6098 - loss_gen: -6.8676\n",
            "Epoch 117/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.0592 - loss_gen: -5.7643\n",
            "Epoch 118/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -11.9992 - loss_gen: -6.0919\n",
            "Epoch 119/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.5426 - loss_gen: -6.0291\n",
            "Epoch 120/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.2274 - loss_gen: -5.6987\n",
            "Epoch 121/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.0834 - loss_gen: -5.9189\n",
            "Epoch 122/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.1400 - loss_gen: -5.8705\n",
            "Epoch 123/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 116.8461 - loss_gen: -6.4652\n",
            "Epoch 124/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.6090 - loss_gen: -6.7602\n",
            "Epoch 125/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.5568 - loss_gen: -6.6792\n",
            "Epoch 126/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.5637 - loss_gen: -6.6889\n",
            "Epoch 127/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.3480 - loss_gen: -6.5412\n",
            "Epoch 128/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.9318 - loss_gen: -6.5009\n",
            "Epoch 129/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.7597 - loss_gen: -5.9720\n",
            "Epoch 130/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.7304 - loss_gen: -5.4683\n",
            "Epoch 131/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.0119 - loss_gen: -5.7276\n",
            "Epoch 132/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.2971 - loss_gen: -4.9509\n",
            "Epoch 133/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.0682 - loss_gen: -5.2591\n",
            "Epoch 134/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.2049 - loss_gen: -4.9971\n",
            "Epoch 135/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.3666 - loss_gen: -5.1966\n",
            "Epoch 136/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.9616 - loss_gen: -4.8810\n",
            "Epoch 137/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.2918 - loss_gen: -5.8836\n",
            "Epoch 138/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.0952 - loss_gen: -4.8649\n",
            "Epoch 139/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -12.6511 - loss_gen: -5.5462\n",
            "Epoch 140/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.0177 - loss_gen: -6.7163\n",
            "Epoch 141/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.0991 - loss_gen: -6.0815\n",
            "Epoch 142/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.3005 - loss_gen: -5.1719\n",
            "Epoch 143/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.3243 - loss_gen: -4.7637\n",
            "Epoch 144/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.6303 - loss_gen: -5.8034\n",
            "Epoch 145/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.1501 - loss_gen: -5.4171\n",
            "Epoch 146/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.1529 - loss_gen: -6.9636\n",
            "Epoch 147/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.0827 - loss_gen: -6.2523\n",
            "Epoch 148/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.2719 - loss_gen: -5.2555\n",
            "Epoch 149/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.1618 - loss_gen: -4.4001\n",
            "Epoch 150/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.9204 - loss_gen: -4.4895\n",
            "Epoch 151/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.4838 - loss_gen: -4.1063\n",
            "Epoch 152/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.7101 - loss_gen: -4.0374\n",
            "Epoch 153/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.9383 - loss_gen: -4.4346\n",
            "Epoch 154/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.6486 - loss_gen: -4.1666\n",
            "Epoch 155/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.3159 - loss_gen: -4.0558\n",
            "Epoch 156/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.5338 - loss_gen: -3.9920\n",
            "Epoch 157/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.7308 - loss_gen: -3.6719\n",
            "Epoch 158/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.2090 - loss_gen: -4.0015\n",
            "Epoch 159/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.2836 - loss_gen: -4.0241\n",
            "Epoch 160/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -9.4677 - loss_gen: -3.4349\n",
            "Epoch 161/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -13.4757 - loss_gen: -5.5160\n",
            "Epoch 162/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.8419 - loss_gen: -5.2780\n",
            "Epoch 163/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.8198 - loss_gen: -4.8931\n",
            "Epoch 164/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.9035 - loss_gen: -5.2338\n",
            "Epoch 165/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.8258 - loss_gen: -5.1771\n",
            "Epoch 166/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.2972 - loss_gen: -5.9186\n",
            "Epoch 167/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.9324 - loss_gen: -5.1979\n",
            "Epoch 168/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.6034 - loss_gen: -5.3238\n",
            "Epoch 169/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.4682 - loss_gen: -4.5886\n",
            "Epoch 170/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.8751 - loss_gen: -4.8159\n",
            "Epoch 171/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.1544 - loss_gen: -5.1759\n",
            "Epoch 172/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.4323 - loss_gen: -4.2364\n",
            "Epoch 173/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.4795 - loss_gen: -4.4452\n",
            "Epoch 174/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.2065 - loss_gen: -4.2570\n",
            "Epoch 175/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.7720 - loss_gen: -3.7721\n",
            "Epoch 176/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.3502 - loss_gen: -4.0740\n",
            "Epoch 177/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -17.4088 - loss_gen: -4.3036\n",
            "Epoch 178/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.4514 - loss_gen: -2.6497\n",
            "Epoch 179/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: 19.4071 - loss_gen: -5.2752\n",
            "Epoch 180/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.0565 - loss_gen: -4.4553\n",
            "Epoch 181/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.3394 - loss_gen: -5.5730\n",
            "Epoch 182/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.6685 - loss_gen: -4.2754\n",
            "Epoch 183/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.1823 - loss_gen: -4.4577\n",
            "Epoch 184/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.6407 - loss_gen: -3.7507\n",
            "Epoch 185/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -16.0923 - loss_gen: -3.0673\n",
            "Epoch 186/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.4326 - loss_gen: -4.1568\n",
            "Epoch 187/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.3260 - loss_gen: -4.1310\n",
            "Epoch 188/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -14.8948 - loss_gen: -3.2590\n",
            "Epoch 189/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -17.0787 - loss_gen: -3.5923\n",
            "Epoch 190/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -52.6629 - loss_gen: -2.7026\n",
            "Epoch 191/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.4899 - loss_gen: 43.8495\n",
            "Epoch 192/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.2430 - loss_gen: -3.3558\n",
            "Epoch 193/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.5177 - loss_gen: -3.8573\n",
            "Epoch 194/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.8667 - loss_gen: -3.7374\n",
            "Epoch 195/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.7131 - loss_gen: -3.3635\n",
            "Epoch 196/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -16.2332 - loss_gen: -3.3990\n",
            "Epoch 197/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.6652 - loss_gen: -2.5371\n",
            "Epoch 198/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -15.8354 - loss_gen: -3.9066\n",
            "Epoch 199/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -16.0322 - loss_gen: -3.8460\n",
            "Epoch 200/200\n",
            "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0000e+00 - loss_dis: -16.3286 - loss_gen: -3.7753\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a4ef4502d90>"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an instance of the GraphGenerator and assign it to the variable 'generator'\n",
        "generator = GraphGenerator(\n",
        "    dense_units=[512, 512],\n",
        "    dropout_rate=0.2,\n",
        "    latent_dim=LATENT_DIM,\n",
        "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
        "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
        ")\n",
        "\n",
        "wgan = GraphWGAN(generator, discriminator, discriminator_steps=1)\n",
        "\n",
        "wgan.compile(\n",
        "    optimizer_generator=keras.optimizers.Adam(1e-4),\n",
        "    optimizer_discriminator=keras.optimizers.Adam(1e-4),\n",
        ")\n",
        "\n",
        "wgan.fit([adjacency_tensor, feature_tensor], epochs=200, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "9gO_-_w9b-LJ",
        "outputId": "700504d7-1fba-4542-9873-3253232d6926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:rdkit=\"http://www.rdkit.org/xml\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.1\" baseProfile=\"full\" xml:space=\"preserve\" width=\"750px\" height=\"150px\" viewBox=\"0 0 750 150\">\n<!-- END OF HEADER -->\n<rect style=\"opacity:1.0;fill:#FFFFFF;stroke:none\" width=\"750.0\" height=\"150.0\" x=\"0.0\" y=\"0.0\"> </rect>\n<path class=\"bond-0 atom-0 atom-7\" d=\"M 141.9,94.0 L 143.2,60.0\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-1 atom-1 atom-2\" d=\"M 6.8,74.6 L 24.9,45.8\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-2 atom-2 atom-5\" d=\"M 24.9,45.8 L 59.0,42.0\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-3 atom-4 atom-5\" d=\"M 89.1,26.1 L 59.0,42.0\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-4 atom-4 atom-8\" d=\"M 89.1,26.1 L 122.3,33.2\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-5 atom-5 atom-10\" d=\"M 59.0,42.0 L 53.8,54.8\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-5 atom-5 atom-10\" d=\"M 53.8,54.8 L 48.7,67.5\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-6 atom-7 atom-8\" d=\"M 143.2,60.0 L 122.3,33.2\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-7 atom-9 atom-10\" d=\"M 56.7,105.9 L 52.5,92.7\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-7 atom-9 atom-10\" d=\"M 52.5,92.7 L 48.2,79.6\" style=\"fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-8 atom-0 atom-6\" d=\"M 141.9,94.0 L 119.2,119.2\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-8 atom-0 atom-6\" d=\"M 136.9,92.0 L 115.4,115.8\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-9 atom-1 atom-3\" d=\"M 6.8,74.6 L 13.2,86.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-9 atom-1 atom-3\" d=\"M 13.2,86.6 L 19.6,98.7\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-9 atom-1 atom-3\" d=\"M 12.7,74.8 L 18.4,85.5\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-9 atom-1 atom-3\" d=\"M 18.4,85.5 L 24.1,96.3\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-10 atom-3 atom-9\" d=\"M 27.5,104.8 L 42.1,105.3\" style=\"fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-10 atom-3 atom-9\" d=\"M 42.1,105.3 L 56.7,105.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-11 atom-6 atom-11\" d=\"M 119.2,119.2 L 85.5,123.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-12 atom-9 atom-11\" d=\"M 56.7,105.9 L 85.5,123.9\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path class=\"bond-12 atom-9 atom-11\" d=\"M 55.1,110.9 L 86.6,118.6\" style=\"fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\"/>\n<path d=\"M 142.0,92.3 L 141.9,94.0 L 140.8,95.3\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 7.7,73.1 L 6.8,74.6 L 7.1,75.2\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 24.0,47.2 L 24.9,45.8 L 26.6,45.6\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 87.6,26.9 L 89.1,26.1 L 90.7,26.5\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 120.3,118.0 L 119.2,119.2 L 117.5,119.5\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 143.1,61.7 L 143.2,60.0 L 142.1,58.7\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 120.6,32.9 L 122.3,33.2 L 123.3,34.6\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path d=\"M 87.2,123.7 L 85.5,123.9 L 84.1,123.0\" style=\"fill:none;stroke:#000000;stroke-width:2.0px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;\"/>\n<path class=\"atom-3\" d=\"M 20.6 99.8 L 23.8 104.9 Q 24.1 105.4, 24.6 106.3 Q 25.1 107.2, 25.1 107.3 L 25.1 99.8 L 26.4 99.8 L 26.4 109.4 L 25.1 109.4 L 21.7 103.9 Q 21.3 103.2, 20.9 102.5 Q 20.5 101.7, 20.3 101.5 L 20.3 109.4 L 19.1 109.4 L 19.1 99.8 L 20.6 99.8 \" fill=\"#0000FF\"/>\n<path class=\"atom-10\" d=\"M 41.8 73.5 Q 41.8 71.2, 43.0 69.9 Q 44.1 68.6, 46.2 68.6 Q 48.4 68.6, 49.5 69.9 Q 50.7 71.2, 50.7 73.5 Q 50.7 75.9, 49.5 77.2 Q 48.4 78.5, 46.2 78.5 Q 44.1 78.5, 43.0 77.2 Q 41.8 75.9, 41.8 73.5 M 46.2 77.4 Q 47.7 77.4, 48.5 76.5 Q 49.3 75.5, 49.3 73.5 Q 49.3 71.6, 48.5 70.7 Q 47.7 69.7, 46.2 69.7 Q 44.8 69.7, 44.0 70.7 Q 43.2 71.6, 43.2 73.5 Q 43.2 75.5, 44.0 76.5 Q 44.8 77.4, 46.2 77.4 \" fill=\"#FF0000\"/>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sample(generator, batch_size):\n",
        "    z = tf.random.normal((batch_size, LATENT_DIM))\n",
        "    graph = generator.predict(z)\n",
        "    # obtain one-hot encoded adjacency tensor\n",
        "    adjacency = tf.argmax(graph[0], axis=1)\n",
        "    adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
        "    # Remove potential self-loops from adjacency\n",
        "    adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
        "    # obtain one-hot encoded feature tensor\n",
        "    features = tf.argmax(graph[1], axis=2)\n",
        "    features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
        "    return [\n",
        "        graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
        "        for i in range(batch_size)\n",
        "    ]\n",
        "\n",
        "\n",
        "molecules = sample(wgan.generator, batch_size=48)\n",
        "\n",
        "MolsToGridImage(\n",
        "    [m for m in molecules if m is not None][:25], molsPerRow=5, subImgSize=(150, 150)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n8r7YAecBlL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# save molecules as smiles in a txt file\n",
        "output_filename = \"generated_smiles.txt\"\n",
        "output_directory = \"/content/\"\n",
        "output_path = os.path.join(output_directory, output_filename)\n",
        "\n",
        "try:\n",
        "    with open(output_path, 'w') as f:\n",
        "        for mol in molecules:\n",
        "            if mol is not None and isinstance(mol, Chem.Mol):\n",
        "                try:\n",
        "                    smiles = Chem.MolToSmiles(mol)\n",
        "                    f.write(smiles + \"\\n\") #add write statement\n",
        "                except:\n",
        "                    pass # Handle the exception as needed\n",
        "except NameError:\n",
        "    pass # Handle the exception as needed\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "output_directory = \"/content/\"\n",
        "output_path = os.path.join(output_directory, output_filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}